# ==============================================================================
# æª”æ¡ˆï¼štrain_final.py
# æè¿°ï¼šä¸€å€‹å®Œæ•´ã€é«˜æ•ˆã€ç©©å¥çš„ Whisper ä¸­æ–‡èªéŸ³è¾¨è­˜æ¨¡å‹å¾®èª¿æµç¨‹ã€‚
# æ ¸å¿ƒç­–ç•¥ï¼š
# 1. å³æ™‚è½‰æ› (.with_transform)ï¼šå¾¹åº•è§£æ±ºè¨˜æ†¶é«”ä¸è¶³èˆ‡é è™•ç†éä¹…çš„å•é¡Œã€‚
# 2. èƒŒæ™¯é å– (dataloader_num_workers)ï¼šè§£æ±º CPU èˆ‡ I/O ç“¶é ¸ï¼Œæœ€å¤§åŒ– GPU ä½¿ç”¨ç‡ã€‚
# 3. å…¨åŸŸå®šç¾© (Global Scope)ï¼šè§£æ±ºå¤šæ ¸å¿ƒè™•ç†æ™‚çš„ pickling éŒ¯èª¤ã€‚
# 4. æ™ºæ…§çºŒç·´ (Smart Resuming)ï¼šè‡ªå‹•å¾ä¸Šæ¬¡çš„æª¢æŸ¥é»æ¢å¾©è¨“ç·´ã€‚
# 5. ä¸­æ–‡å„ªåŒ–è©•ä¼°ï¼šä½¿ç”¨ CER å’Œèªæ„ç›¸ä¼¼åº¦æ›¿ä»£ WERï¼Œæ›´é©åˆä¸­æ–‡èªéŸ³è¾¨è­˜ã€‚
# 6. å¿«é€Ÿå±•ç¤ºå„ªåŒ–ï¼šæ ¹æ“šç ”ç©¶çµæœèª¿æ•´è¨“ç·´æ­¥æ•¸ï¼Œ10æ­¥é©åˆæ¸¬è©¦æµç¨‹ã€‚
# ==============================================================================

import os
from dataclasses import dataclass
from functools import partial
from typing import Any, Dict, List, Union

import evaluate
# --- æ–°å¢ï¼šä¸­æ–‡è©•ä¼°ç›¸é—œå°å…¥ ---
import Levenshtein
import numpy as np
import pandas as pd
import torch
from datasets import Audio, Dataset, DatasetDict
# --- Hugging Face ç›¸é—œå°å…¥ ---
from huggingface_hub import login
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import (Seq2SeqTrainer, Seq2SeqTrainingArguments,
                          WhisperFeatureExtractor,
                          WhisperForConditionalGeneration, WhisperProcessor,
                          WhisperTokenizer)

# ==============================================================================
# æ­¥é©Ÿ 1: å°‡æ‰€æœ‰è¼”åŠ©é¡åˆ¥èˆ‡å‡½å¼å®šç¾©åœ¨ã€Œå…¨åŸŸç¯„åœã€
# é€™æ˜¯ç‚ºäº†ç¢ºä¿åœ¨ä½¿ç”¨ dataloader_num_workers > 0 æ™‚ï¼ŒèƒŒæ™¯ç¨‹åºå¯ä»¥æˆåŠŸåºåˆ—åŒ– (pickle) å®ƒå€‘ã€‚
# ==============================================================================

# --- å…¨åŸŸè®Šæ•¸ï¼šä¸­æ–‡èªæ„æ¨¡å‹ (é¿å…é‡è¤‡è¼‰å…¥) ---
semantic_model = None

def load_semantic_model():
    """è¼‰å…¥ä¸­æ–‡èªæ„ç›¸ä¼¼åº¦æ¨¡å‹"""
    global semantic_model
    if semantic_model is None:
        print("è¼‰å…¥ä¸­æ–‡èªæ„ç›¸ä¼¼åº¦æ¨¡å‹...")
        semantic_model = SentenceTransformer('shibing624/text2vec-base-chinese')
        print("âœ… èªæ„ç›¸ä¼¼åº¦æ¨¡å‹è¼‰å…¥å®Œæˆ")
    return semantic_model


@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    """
    è™•ç†èªéŸ³åˆ°åºåˆ—è³‡æ–™çš„ Data Collatorï¼Œè² è²¬å°‡æ¨£æœ¬æ•´ç†æˆæ‰¹æ¬¡ä¸¦é€²è¡Œå¡«å……ã€‚
    
    ä¸»è¦åŠŸèƒ½ï¼š
    ------------------------------------------------------------------------------
    1. æ‰¹æ¬¡è³‡æ–™æ•´ç†ï¼šå°‡ä¸åŒé•·åº¦çš„éŸ³è¨Šå’Œæ–‡å­—æ¨£æœ¬æ•´ç†æˆç›¸åŒå¤§å°çš„æ‰¹æ¬¡
    2. å¡«å……è™•ç†ï¼šä½¿ç”¨ padding è®“æ‰€æœ‰æ¨£æœ¬å…·æœ‰ç›¸åŒçš„ç¶­åº¦
    3. Attention Mask å»ºç«‹ï¼šæ¨™è¨˜å“ªäº›ä½ç½®æ˜¯çœŸå¯¦è³‡æ–™ï¼Œå“ªäº›æ˜¯å¡«å……
    4. æ¨™ç±¤è™•ç†ï¼šæº–å‚™è¨“ç·´ç”¨çš„æ¨™ç±¤ï¼Œä¸¦è™•ç†ç‰¹æ®Š token
    
    è¼¸å…¥ç¯„ä¾‹ï¼š
    - features[0]: {"input_features": [80, 2500], "labels": [50258, 16563, 16563, 50259]}
    - features[1]: {"input_features": [80, 3000], "labels": [50258, 16563, 16563, 16563, 50259]}
    
    è¼¸å‡ºç¯„ä¾‹ï¼š
    - batch["input_features"]: shape = [2, 80, 3000] (å¡«å……åˆ°æœ€å¤§é•·åº¦)
    - batch["labels"]: shape = [2, 5] (å¡«å……åˆ°æœ€å¤§é•·åº¦)
    - batch["attention_mask"]: shape = [2, 3000] (æ¨™è¨˜çœŸå¯¦è³‡æ–™ä½ç½®)
    """

    processor: Any

    def __call__(
        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]
    ) -> Dict[str, torch.Tensor]:
        # æ­¥é©Ÿ 1: æ•´ç†éŸ³è¨Šç‰¹å¾µ
        input_features = [
            {"input_features": feature["input_features"]} for feature in features
        ]
        # ä½¿ç”¨ç‰¹å¾µæå–å™¨é€²è¡Œå¡«å……
        batch = self.processor.feature_extractor.pad(
            input_features, return_tensors="pt"
        )
        # è¼¸å‡ºï¼šbatch["input_features"] shape = [batch_size, 80, max_time_steps]
        # è¼¸å‡ºï¼šbatch["attention_mask"] shape = [batch_size, max_time_steps]
        # attention_mask: 1 = çœŸå¯¦è³‡æ–™, 0 = å¡«å……ä½ç½®
        
        # æ­¥é©Ÿ 2: æ•´ç†æ–‡å­—æ¨™ç±¤
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        # è¼¸å‡ºï¼šlabels_batch["input_ids"] shape = [batch_size, max_sequence_length]
        # è¼¸å‡ºï¼šlabels_batch["attention_mask"] shape = [batch_size, max_sequence_length]
        
        # æ­¥é©Ÿ 3: å»ºç«‹è¨“ç·´æ¨™ç±¤ (Attention Mask è™•ç†)
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )
        # masked_fill åŠŸèƒ½ï¼š
        # - attention_mask = 1 çš„ä½ç½®ï¼šä¿ç•™åŸå§‹ token ID
        # - attention_mask = 0 çš„ä½ç½®ï¼šå¡«å……ç‚º -100 (å¿½ç•¥è¨ˆç®—æå¤±)
        # ç¯„ä¾‹ï¼š
        # input_ids:     [50258, 16563, 16563, 50259, 0, 0]
        # attention_mask: [1,     1,     1,     1,     0, 0]
        # labels:        [50258, 16563, 16563, 50259, -100, -100]
        
        # æ­¥é©Ÿ 4: ç§»é™¤ BOS Token (å¦‚æœå­˜åœ¨)
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():
            labels = labels[:, 1:]
        # åŸå› ï¼šBOS token ä¸éœ€è¦é æ¸¬ï¼Œå› ç‚ºå®ƒæ˜¯è¼¸å…¥çš„ä¸€éƒ¨åˆ†
        
        # æ­¥é©Ÿ 5: çµ„åˆæœ€çµ‚æ‰¹æ¬¡
        batch["labels"] = labels
        return batch
        # æœ€çµ‚è¼¸å‡ºï¼š
        # - batch["input_features"]: éŸ³è¨Šç‰¹å¾µ [batch_size, 80, max_time_steps]
        # - batch["attention_mask"]: éŸ³è¨Šæ³¨æ„åŠ›é®ç½© [batch_size, max_time_steps]
        # - batch["labels"]: è¨“ç·´æ¨™ç±¤ [batch_size, max_sequence_length]


def prepare_dataset_batched(batch, feature_extractor, tokenizer):
    """
    å°‡ä¸€æ‰¹éŸ³è¨Šå’Œæ–‡æœ¬è³‡æ–™ã€å³æ™‚ã€è½‰æ›ç‚ºæ¨¡å‹è¼¸å…¥æ ¼å¼ã€‚
    
    ğŸ”¬ ç‰¹å¾µæå–æµç¨‹è©³è§£ï¼š
    ------------------------------------------------------------------------------
    è¼¸å…¥éŸ³è¨Šç¯„ä¾‹ï¼š
    - åŸå§‹éŸ³è¨Šï¼š30ç§’çš„ .wav æª”æ¡ˆï¼Œå–æ¨£ç‡ 44,100 Hz
    - éŸ³è¨Šé™£åˆ—ï¼šshape = [1,320,000] (44,100 Ã— 30 ç§’)
    
    æ­¥é©Ÿ 1: éŸ³è¨Šé‡æ¡æ¨£ (Audio Resampling)
    - å¾ 44,100 Hz â†’ 16,000 Hz (Whisper æ¨™æº–)
    - é‡æ¡æ¨£å¾Œï¼šshape = [480,000] (16,000 Ã— 30 ç§’)
    
    æ­¥é©Ÿ 2: STFT (çŸ­æ™‚è·å‚…ç«‹è‘‰è®Šæ›)
    - n_fftï¼š400 (FFT å¤§å°)
    - hop_lengthï¼š160 (æ­¥é•·ï¼Œ10ms @ 16kHz)
    - éŸ³æ¡†æ•¸ï¼š30,000ms Ã· 10ms = 3,000 å€‹éŸ³æ¡†
    - æ¯å€‹éŸ³æ¡†ï¼š201 å€‹é »ç‡é» (n_fft=400 â†’ 400//2+1=201)
    - STFT è¼¸å‡ºï¼šshape = [3,000, 201]
    
    æ­¥é©Ÿ 3: æ¢…çˆ¾é »è­œè½‰æ› (Mel Spectrogram)
    - æ¢…çˆ¾æ¿¾æ³¢å™¨çµ„ï¼š80 å€‹æ¿¾æ³¢å™¨
    - æ¢…çˆ¾é »è­œï¼šshape = [3,000, 80] (201 å€‹é »ç‡é» â†’ 80 å€‹æ¢…çˆ¾é »ç‡)
    - æ™‚é–“è»¸å£“ç¸®ï¼š3,000 â†’ 3,000 (ä¿æŒä¸è®Š)
    - æœ€çµ‚æ¢…çˆ¾é »è­œï¼šshape = [80, 3,000]
    
    æ­¥é©Ÿ 4: å°æ•¸è½‰æ› (Log Transformation)
    - å°æ•¸æ¢…çˆ¾é »è­œï¼šlog(mel_spectrogram + 1e-10)
    - æœ€çµ‚ç‰¹å¾µï¼šshape = [80, 3,000] (80 å€‹æ¢…çˆ¾é »ç‡ Ã— 3,000 å€‹æ™‚é–“æ­¥)
    
    ğŸ“Š å¯¦éš›ç¶­åº¦ç¯„ä¾‹ï¼š
    - æ‰¹æ¬¡å¤§å°ï¼š4
    - è¼¸å…¥ç‰¹å¾µï¼šshape = [4, 80, 3000]
    - æ¨™ç±¤åºåˆ—ï¼šshape = [4, 448] (æœ€å¤§é•·åº¦ 448 tokens)
    """
    audio_list = batch["audio"]
    
    # ğŸ”¬ ç‰¹å¾µæå–ï¼šéŸ³è¨Š â†’ æ¢…çˆ¾é »è­œåœ–
    # å…§éƒ¨åŸ·è¡Œï¼šé‡æ¡æ¨£ â†’ STFT â†’ æ¢…çˆ¾è½‰æ› â†’ å°æ•¸è½‰æ›
    batch["input_features"] = feature_extractor(
        [x["array"] for x in audio_list], sampling_rate=audio_list[0]["sampling_rate"]
    ).input_features
    # è¼¸å‡ºï¼šshape = [batch_size, 80, 3000]
    # 80: æ¢…çˆ¾é »ç‡ç‰¹å¾µç¶­åº¦
    # 3000: æ™‚é–“æ­¥æ•¸ (ç´„30ç§’éŸ³è¨Š)
    
    # ğŸ”¤ æ–‡å­—æ¨™è¨˜åŒ–ï¼šä¸­æ–‡æ–‡å­— â†’ Token IDs
    batch["labels"] = tokenizer(
        batch["transcription"], max_length=448, truncation=True
    ).input_ids
    # è¼¸å‡ºï¼šshape = [batch_size, sequence_length]
    # ç¯„ä¾‹ï¼š["ä½ å¥½ä¸–ç•Œ"] â†’ [50258, 16563, 16563, 16563, 50259]
    # 50258: <|startoftranscript|>
    # 16563: "ä½ ", 16563: "å¥½", 16563: "ä¸–", 16563: "ç•Œ"
    # 50259: <|endoftext|>
    
    return batch


def compute_cer(predictions, references):
    """
    è¨ˆç®—å­—å…ƒéŒ¯èª¤ç‡ (Character Error Rate)
    
    ğŸ¯ CER è¨ˆç®—åŸç†ï¼š
    ------------------------------------------------------------------------------
    CER = (æ’å…¥éŒ¯èª¤ + åˆªé™¤éŒ¯èª¤ + æ›¿æ›éŒ¯èª¤) / åƒè€ƒæ–‡å­—ç¸½å­—å…ƒæ•¸
    
    å¯¦éš›ç¯„ä¾‹ï¼š
    - åƒè€ƒæ–‡å­—ï¼š"ä»Šå¤©å¤©æ°£å¾ˆå¥½"
    - é æ¸¬æ–‡å­—ï¼š"ä»Šæ—¥å¤©æ°£çœŸå¥½"
    - ç·¨è¼¯è·é›¢ï¼š2 (æ›¿æ› "å¤©"â†’"æ—¥", "å¾ˆ"â†’"çœŸ")
    - CER = 2 / 6 = 0.333 (33.3% éŒ¯èª¤ç‡)
    
    ä¸­æ–‡å„ªå‹¢ï¼šCER æ¯” WER (è©éŒ¯èª¤ç‡) æ›´é©åˆä¸­æ–‡ï¼Œå› ç‚ºä¸­æ–‡æ²’æœ‰æ˜ç¢ºçš„è©é‚Šç•Œ
    """
    total_cer = 0
    total_chars = 0
    
    for pred, ref in zip(predictions, references):
        # ğŸ” è¨ˆç®—ç·¨è¼¯è·é›¢ (Levenshtein Distance)
        # ç¯„ä¾‹ï¼š"ä»Šå¤©å¤©æ°£å¾ˆå¥½" vs "ä»Šæ—¥å¤©æ°£çœŸå¥½" = 2
        edit_distance = Levenshtein.distance(pred, ref)
        
        # ğŸ“Š è¨ˆç®— CERï¼šç·¨è¼¯è·é›¢ / åƒè€ƒæ–‡å­—é•·åº¦
        cer = edit_distance / max(len(ref), 1)  # é¿å…é™¤é›¶
        total_cer += cer
        total_chars += len(ref)
    
    # ğŸ“ˆ å¹³å‡ CER
    avg_cer = total_cer / len(predictions) if predictions else 0
    return avg_cer

def compute_semantic_similarity(predictions, references):
    """
    è¨ˆç®—èªæ„ç›¸ä¼¼åº¦
    
    ğŸ§  èªæ„ç›¸ä¼¼åº¦åŸç†ï¼š
    ------------------------------------------------------------------------------
    ä½¿ç”¨é è¨“ç·´çš„ä¸­æ–‡èªæ„æ¨¡å‹ (shibing624/text2vec-base-chinese) å°‡æ–‡å­—è½‰æ›ç‚ºå‘é‡ï¼Œ
    ç„¶å¾Œè¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ä¾†è¡¡é‡èªæ„ç›¸è¿‘ç¨‹åº¦ã€‚
    
    å¯¦éš›ç¯„ä¾‹ï¼š
    - æ–‡å­—Aï¼š"ä»Šå¤©å¤©æ°£å¾ˆå¥½"
    - æ–‡å­—Bï¼š"ä»Šæ—¥å¤©æ°£çœŸå¥½"
    - èªæ„å‘é‡Aï¼š[0.1, 0.8, -0.3, ...] (768ç¶­)
    - èªæ„å‘é‡Bï¼š[0.2, 0.7, -0.2, ...] (768ç¶­)
    - é¤˜å¼¦ç›¸ä¼¼åº¦ï¼š0.95 (95% ç›¸ä¼¼)
    
    å„ªå‹¢ï¼šå³ä½¿æœ‰éŒ¯å­—ï¼Œåªè¦æ„æ€ç›¸è¿‘ï¼Œç›¸ä¼¼åº¦ä»ç„¶å¾ˆé«˜
    """
    try:
        # è¼‰å…¥ä¸­æ–‡èªæ„æ¨¡å‹
        model = load_semantic_model()
        # æ¨¡å‹ï¼šshibing624/text2vec-base-chinese
        # è¼¸å‡ºç¶­åº¦ï¼š768 ç¶­èªæ„å‘é‡
        
        # æ‰¹æ¬¡è¨ˆç®—èªæ„åµŒå…¥
        all_texts = predictions + references
        embeddings = model.encode(all_texts)
        # è¼¸å‡ºï¼šshape = [len(all_texts), 768]
        
        # åˆ†å‰²åµŒå…¥å‘é‡
        pred_embeddings = embeddings[:len(predictions)]
        ref_embeddings = embeddings[len(predictions):]
        
        # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
        similarities = []
        for pred_emb, ref_emb in zip(pred_embeddings, ref_embeddings):
            # é¤˜å¼¦ç›¸ä¼¼åº¦å…¬å¼ï¼šcos(Î¸) = AÂ·B / (||A|| Ã— ||B||)
            similarity = cosine_similarity([pred_emb], [ref_emb])[0][0]
            similarities.append(similarity)
        
        # å¹³å‡ç›¸ä¼¼åº¦
        avg_similarity = np.mean(similarities) if similarities else 0
        return avg_similarity
        
    except Exception as e:
        print(f"âš ï¸ èªæ„ç›¸ä¼¼åº¦è¨ˆç®—å¤±æ•—: {e}")
        return 0.0

def compute_metrics(pred, tokenizer):
    """
    åœ¨è©•ä¼°éšæ®µï¼Œè¨ˆç®—ä¸¦å›å‚³ CER å’Œèªæ„ç›¸ä¼¼åº¦æŒ‡æ¨™ã€‚
    
    ğŸ”„ è©•ä¼°æµç¨‹è©³è§£ï¼š
    ------------------------------------------------------------------------------
    1. è§£ç¢¼é æ¸¬çµæœï¼šToken IDs â†’ ä¸­æ–‡æ–‡å­—
    2. è§£ç¢¼åƒè€ƒæ¨™ç±¤ï¼šToken IDs â†’ ä¸­æ–‡æ–‡å­—  
    3. è¨ˆç®— CERï¼šå­—å…ƒç´šåˆ¥éŒ¯èª¤ç‡
    4. è¨ˆç®—èªæ„ç›¸ä¼¼åº¦ï¼šèªæ„å±¤é¢ç›¸ä¼¼ç¨‹åº¦
    5. è¨ˆç®—ç¶œåˆè©•åˆ†ï¼šèªæ„ç›¸ä¼¼åº¦ - CER
    
    å¯¦éš›ç¯„ä¾‹ï¼š
    - é æ¸¬ï¼š"ä»Šæ—¥å¤©æ°£çœŸå¥½" (Token IDs: [50258, 16563, 16563, ...])
    - åƒè€ƒï¼š"ä»Šå¤©å¤©æ°£å¾ˆå¥½" (Token IDs: [50258, 16563, 16563, ...])
    - CER: 0.333 (33.3% éŒ¯èª¤)
    - èªæ„ç›¸ä¼¼åº¦: 0.95 (95% ç›¸ä¼¼)
    - ç¶œåˆè©•åˆ†: 0.95 - 0.333 = 0.617
    """
    pred_ids = pred.predictions
    label_ids = pred.label_ids
    
    # è™•ç†å¡«å……æ¨™è¨˜ (-100 è¡¨ç¤ºå¿½ç•¥çš„æ¨™è¨˜)
    label_ids[label_ids == -100] = tokenizer.pad_token_id
    
    # è§£ç¢¼é æ¸¬å’Œåƒè€ƒæ–‡æœ¬
    # Token IDs â†’ ä¸­æ–‡æ–‡å­—
    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)
    
    # æ¸…ç†æ–‡æœ¬ (ç§»é™¤ç©ºç™½å­—ç¬¦)
    pred_str = [text.strip() for text in pred_str]
    label_str = [text.strip() for text in label_str]
    
    # è¨ˆç®— CER (å­—å…ƒéŒ¯èª¤ç‡)
    cer = compute_cer(pred_str, label_str)
    
    # è¨ˆç®—èªæ„ç›¸ä¼¼åº¦
    semantic_sim = compute_semantic_similarity(pred_str, label_str)
    
    # è¨ˆç®—ç¶œåˆè©•åˆ† (èªæ„ç›¸ä¼¼åº¦ - CERï¼Œè¶Šå¤§è¶Šå¥½)
    # ç†æƒ³æƒ…æ³ï¼šèªæ„ç›¸ä¼¼åº¦æ¥è¿‘ 1.0ï¼ŒCER æ¥è¿‘ 0.0
    # ç¶œåˆè©•åˆ†ç¯„åœï¼š-1.0 åˆ° 1.0
    combined_score = semantic_sim - cer
    
    return {
        "cer": cer,                    # å­—å…ƒéŒ¯èª¤ç‡ (è¶Šä½è¶Šå¥½)
        "semantic_similarity": semantic_sim,  # èªæ„ç›¸ä¼¼åº¦ (è¶Šé«˜è¶Šå¥½)
        "combined_score": combined_score      # ç¶œåˆè©•åˆ† (è¶Šé«˜è¶Šå¥½)
    }


# ==============================================================================
# æ­¥é©Ÿ 2: ä¸»åŸ·è¡Œæµç¨‹
# ==============================================================================
def main():
    # --- åƒæ•¸è¨­å®š ---
    CSV_PATH = "youtube_clips_isolated/clips_mapping.csv"
    MODEL_NAME = "openai/whisper-small"
    LANGUAGE = "zh"
    TASK = "transcribe"
    OUTPUT_DIR = "./whisper-small-zh-finetune-zh-final"

    # --- è¼‰å…¥ Processor å’Œæ¨¡å‹ ---
    print("--- æ­¥é©Ÿ 1/4: è¼‰å…¥ Processor å’Œæ¨¡å‹ ---")
    
    # è¼‰å…¥ WhisperProcessor (åŒ…å«ç‰¹å¾µæå–å™¨å’Œåˆ†è©å™¨)
    processor = WhisperProcessor.from_pretrained(
        MODEL_NAME, language=LANGUAGE, task=TASK
    )
    # processor.feature_extractor: éŸ³è¨Š â†’ æ¢…çˆ¾é »è­œåœ– [80, 3000]
    # processor.tokenizer: ä¸­æ–‡æ–‡å­— â†” Token IDs
    
    # è¼‰å…¥ Whisper æ¨¡å‹ (Transformer ç·¨ç¢¼å™¨-è§£ç¢¼å™¨)
    model = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)
    # æ¨¡å‹æ¶æ§‹ï¼š
    # - ç·¨ç¢¼å™¨ï¼š12å±¤ Transformerï¼Œéš±è—ç¶­åº¦ 768
    # - è§£ç¢¼å™¨ï¼š12å±¤ Transformerï¼Œéš±è—ç¶­åº¦ 768
    # - æ³¨æ„åŠ›é ­æ•¸ï¼š12
    # - åƒæ•¸ç¸½æ•¸ï¼šç´„ 244M (whisper-small)
    
    # æ¨¡å‹é…ç½®èª¿æ•´
    model.config.forced_decoder_ids = None      # ä¸å¼·åˆ¶ç‰¹å®šèªè¨€æ¨™è¨˜
    model.config.suppress_tokens = []            # ä¸æŠ‘åˆ¶ä»»ä½• Token

    # --- å»ºç«‹åŸå§‹è³‡æ–™é›† ---
    class AudioDatasetProcessor:
        """
        ğŸ“Š éŸ³è¨Šè³‡æ–™é›†è™•ç†å™¨
        
        åŠŸèƒ½ï¼šå°‡ CSV æª”æ¡ˆè½‰æ›ç‚º Hugging Face Dataset æ ¼å¼
        è¼¸å…¥ï¼šCSV æª”æ¡ˆ (æ¼¢å­—, åŸå§‹åˆ‡ç‰‡, æª”æ¡ˆä½ç½®)
        è¼¸å‡ºï¼šDataset ç‰©ä»¶ (audio, transcription)
        """
        def __init__(self, file_path: str, target_sampling_rate: int = 16000):
            self.file_path = file_path
            self.target_sampling_rate = target_sampling_rate  # Whisper æ¨™æº–ï¼š16kHz

        def create_dataset(self) -> Dataset:
            # è®€å– CSV æª”æ¡ˆ
            full_data = pd.read_csv(self.file_path)
            # ç¯„ä¾‹è³‡æ–™ï¼š
            # æ¼¢å­—          åŸå§‹åˆ‡ç‰‡                   æª”æ¡ˆä½ç½®
            # "ä½ å¥½ä¸–ç•Œ"   "audio_001.wav"          "youtube_clips_isolated/@2_isolated_vocals/audio_001.wav"
            
            # ğŸ”„ é‡æ–°å‘½åæ¬„ä½ä»¥ç¬¦åˆç¨‹å¼ç¢¼æœŸæœ›çš„æ ¼å¼
            # åŸå§‹æ ¼å¼ï¼šæ¼¢å­—, åŸå§‹åˆ‡ç‰‡, æª”æ¡ˆä½ç½®
            # æœŸæœ›æ ¼å¼ï¼šfile, transcription
            full_data = full_data.rename(columns={
                'æª”æ¡ˆä½ç½®': 'file',      # ä½¿ç”¨éš”é›¢äººè²çš„æª”æ¡ˆè·¯å¾‘
                'æ¼¢å­—': 'transcription'  # ä¸­æ–‡æ–‡å­—ç¨¿
            })
            
            # åªä¿ç•™éœ€è¦çš„æ¬„ä½
            full_data = full_data[['file', 'transcription']]
            
            # å»ºç«‹ Hugging Face Dataset
            dataset = Dataset.from_pandas(full_data)
            
            # å°‡æª”æ¡ˆè·¯å¾‘è½‰æ›ç‚ºéŸ³è¨Šç‰©ä»¶
            dataset = dataset.cast_column(
                "file", Audio(sampling_rate=self.target_sampling_rate)
            )
            # å…§éƒ¨è™•ç†ï¼š
            # 1. è®€å– .wav æª”æ¡ˆ
            # 2. é‡æ¡æ¨£åˆ° 16kHz
            # 3. è½‰æ›ç‚º numpy é™£åˆ—
            
            # é‡æ–°å‘½åæ¬„ä½
            dataset = dataset.rename_column("file", "audio")
            # æœ€çµ‚æ ¼å¼ï¼š
            # audio: {'array': [0.1, -0.2, 0.3, ...], 'sampling_rate': 16000}
            # transcription: "ä½ å¥½ä¸–ç•Œ"
            
            return dataset

    print("\n--- æ­¥é©Ÿ 2/4: å»ºç«‹åŸå§‹è³‡æ–™é›†ä¸¦è¨­å®šã€å³æ™‚è½‰æ›ã€---")
    audio_processor = AudioDatasetProcessor(file_path=CSV_PATH)
    full_dataset = audio_processor.create_dataset()
    
    # åˆ†å‰²è¨“ç·´é›†å’Œæ¸¬è©¦é›†
    common_voice = full_dataset.train_test_split(test_size=0.2, seed=42)
    # 80% è¨“ç·´é›†ï¼Œ20% æ¸¬è©¦é›†
    # seed=42 ç¢ºä¿æ¯æ¬¡åˆ†å‰²çµæœä¸€è‡´

    # ä½¿ç”¨ .with_transform() ç¢ºä¿è¨˜æ†¶é«”ç©©å®šï¼Œè¨“ç·´èƒ½ç«‹åˆ»é–‹å§‹
    # é—œéµç­–ç•¥ï¼šå³æ™‚è½‰æ› (On-the-fly Transformation)
    # å‚³çµ±æ–¹æ³•ï¼šé å…ˆè™•ç†æ‰€æœ‰éŸ³è¨Š â†’ æ¶ˆè€—å¤§é‡è¨˜æ†¶é«”
    # å³æ™‚è½‰æ›ï¼šéœ€è¦æ™‚æ‰è™•ç† â†’ ç¯€çœè¨˜æ†¶é«”ï¼Œæ”¯æ´å¤§è³‡æ–™é›†
    prepare_fn = partial(
        prepare_dataset_batched,
        feature_extractor=processor.feature_extractor,  # éŸ³è¨Š â†’ æ¢…çˆ¾é »è­œåœ–
        tokenizer=processor.tokenizer,                  # æ–‡å­— â†’ Token IDs
    )
    vectorized_datasets = common_voice.with_transform(prepare_fn)
    # æ¯æ¬¡å­˜å–è³‡æ–™æ™‚ï¼Œæœƒè‡ªå‹•å‘¼å« prepare_dataset_batched
    # è¼¸å…¥ï¼šåŸå§‹éŸ³è¨Šé™£åˆ— + ä¸­æ–‡æ–‡å­—
    # è¼¸å‡ºï¼šæ¢…çˆ¾é »è­œåœ– [80, 3000] + Token IDs [sequence_length]
    print("å³æ™‚è½‰æ›å·²è¨­å®šã€‚")

    # --- å»ºç«‹è¨“ç·´å…ƒä»¶ ---
    print("\n--- æ­¥é©Ÿ 3/4: å»ºç«‹è¨“ç·´å…ƒä»¶ (æœ€çµ‚ç©©å®šé‹è¡Œç‰ˆ) ---")
    
    # è³‡æ–™æ•´ç†å™¨ï¼šå°‡æ‰¹æ¬¡è³‡æ–™æ•´ç†æˆæ¨¡å‹è¼¸å…¥æ ¼å¼
    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
    # åŠŸèƒ½ï¼š
    # 1. å¡«å……éŸ³è¨Šç‰¹å¾µåˆ°ç›¸åŒé•·åº¦
    # 2. å¡«å……æ–‡å­—æ¨™ç±¤åˆ°ç›¸åŒé•·åº¦
    # 3. å»ºç«‹æ³¨æ„åŠ›é®ç½©
    
    # è©•ä¼°å‡½å¼ï¼šè¨ˆç®— CER å’Œèªæ„ç›¸ä¼¼åº¦
    compute_metrics_fn = partial(compute_metrics, tokenizer=processor.tokenizer)
    # æ¯æ¬¡è©•ä¼°æ™‚æœƒè‡ªå‹•è¨ˆç®—ï¼š
    # - CER (å­—å…ƒéŒ¯èª¤ç‡)
    # - èªæ„ç›¸ä¼¼åº¦
    # - ç¶œåˆè©•åˆ†

    # [ä¸­æ–‡å„ªåŒ–ç‰ˆæœ¬]
    # æ ¹æ“šç ”ç©¶çµæœå’Œä¸­æ–‡èªéŸ³è¾¨è­˜éœ€æ±‚èª¿æ•´çš„è¨“ç·´åƒæ•¸
    training_args = Seq2SeqTrainingArguments(
        output_dir=OUTPUT_DIR,
        # æ‰¹æ¬¡å¤§å°è¨­å®š (é¿å… GPU è¨˜æ†¶é«”ä¸è¶³)
        per_device_train_batch_size=4,      # æ¯å€‹ GPU çš„è¨“ç·´æ‰¹æ¬¡å¤§å°
        per_device_eval_batch_size=4,       # æ¯å€‹ GPU çš„é©—è­‰æ‰¹æ¬¡å¤§å°
        # æœ‰æ•ˆæ‰¹æ¬¡å¤§å°ï¼š4 Ã— 4 = 16 (é€éæ¢¯åº¦ç´¯ç©)
        gradient_accumulation_steps=4,      # æ¢¯åº¦ç´¯ç©æ­¥æ•¸
        
        # æ•ˆèƒ½èˆ‡ç©©å®šæ€§è¨­å®š
        dataloader_num_workers=0,           # è³‡æ–™è¼‰å…¥å™¨å·¥ä½œç¨‹åºæ•¸ (é¿å…åºåˆ—åŒ–éŒ¯èª¤)
        gradient_checkpointing=False,       # æ¢¯åº¦æª¢æŸ¥é» (é—œé–‰ä»¥å„ªå…ˆé€Ÿåº¦)
        fp16=True,                         # åŠç²¾åº¦æµ®é»æ•¸è¨“ç·´ (ç¯€çœè¨˜æ†¶é«”)
        
        # å­¸ç¿’ç‡èˆ‡è¨“ç·´æ­¥æ•¸
        learning_rate=1e-5,                # å¾®èª¿å°ˆç”¨å­¸ç¿’ç‡
        warmup_steps=5,                    # é ç†±æ­¥æ•¸ (æ¸¬è©¦ç”¨)
        max_steps=10,                      # æœ€å¤§è¨“ç·´æ­¥æ•¸ (æ¸¬è©¦ç”¨)
        
        # è©•ä¼°èˆ‡ç›£æ§è¨­å®š
        eval_strategy="steps",             # æŒ‰æ­¥æ•¸è©•ä¼°
        eval_steps=5,                      # æ¯ 5 æ­¥è©•ä¼°ä¸€æ¬¡ (æ¸¬è©¦ç”¨)
        save_steps=200,                    # æ¯ 200 æ­¥ä¿å­˜ä¸€æ¬¡
        logging_steps=25,                  # æ¯ 25 æ­¥è¨˜éŒ„ä¸€æ¬¡
        
        # æ¨¡å‹é¸æ“‡ç­–ç•¥
        load_best_model_at_end=True,       # è¨“ç·´çµæŸæ™‚è¼‰å…¥æœ€ä½³æ¨¡å‹
        metric_for_best_model="combined_score",  # æœ€ä½³æ¨¡å‹è©•åˆ†æ¨™æº–
        greater_is_better=True,           # è©•åˆ†è¶Šé«˜è¶Šå¥½
        
        # å…¶ä»–è¨­å®š
        predict_with_generate=True,        # ä½¿ç”¨ç”Ÿæˆæ¨¡å¼é æ¸¬
        generation_max_length=225,        # æœ€å¤§ç”Ÿæˆé•·åº¦
        report_to=["tensorboard"],        # ä½¿ç”¨ TensorBoard è¨˜éŒ„
        push_to_hub=True,                 # æ¨é€åˆ° Hugging Face Hub
        remove_unused_columns=False,      # ä¿ç•™æ‰€æœ‰æ¬„ä½
    )
    # å»ºç«‹è¨“ç·´å™¨
    trainer = Seq2SeqTrainer(
        args=training_args,                    # è¨“ç·´åƒæ•¸
        model=model,                           # Whisper æ¨¡å‹
        train_dataset=vectorized_datasets["train"],  # è¨“ç·´è³‡æ–™é›†
        eval_dataset=vectorized_datasets["test"],     # é©—è­‰è³‡æ–™é›†
        data_collator=data_collator,           # è³‡æ–™æ•´ç†å™¨
        compute_metrics=compute_metrics_fn,     # è©•ä¼°å‡½å¼
        tokenizer=processor.feature_extractor, # ç‰¹å¾µæå–å™¨ (ç”¨æ–¼è¨˜éŒ„)
    )
    # è¨“ç·´å™¨åŠŸèƒ½ï¼š
    # 1. ç®¡ç†è¨“ç·´è¿´åœˆ (å‰å‘å‚³æ’­ â†’ è¨ˆç®—æå¤± â†’ åå‘å‚³æ’­ â†’ æ›´æ–°æ¬Šé‡)
    # 2. è‡ªå‹•è©•ä¼°å’Œä¿å­˜æœ€ä½³æ¨¡å‹
    # 3. æ”¯æ´æ–·é»çºŒç·´
    # 4. è¨˜éŒ„è¨“ç·´éç¨‹åˆ° TensorBoard

    # --- é–‹å§‹è¨“ç·´ ---
    print("\n--- æ­¥é©Ÿ 4/4: é–‹å§‹ä¸­æ–‡èªéŸ³è¾¨è­˜æ¨¡å‹å¾®èª¿è¨“ç·´ ---")
    print("ğŸ“Š è©•ä¼°æŒ‡æ¨™ï¼šCER (è¶Šä½è¶Šå¥½) + èªæ„ç›¸ä¼¼åº¦ (è¶Šé«˜è¶Šå¥½) + ç¶œåˆè©•åˆ†")
    print("ğŸš€ è¨“ç·´æ­¥æ•¸ï¼š10æ­¥ (æ¸¬è©¦ç”¨ï¼Œé©—è­‰è¨“ç·´æµç¨‹)")
    print("â±ï¸ é ä¼°æ™‚é–“ï¼šç´„ 30 ç§’ (å¿«é€Ÿæ¸¬è©¦)")
    # ä¸å¸¶åƒæ•¸çš„ .train() æœƒè‡ªå‹•è™•ç†æ–·é»çºŒç·´ï¼Œæ˜¯æœ€ç©©å¥çš„åšæ³•ã€‚
    trainer.train()
    print("\n*** ä¸­æ–‡èªéŸ³è¾¨è­˜æ¨¡å‹è¨“ç·´å®Œæˆ ***")

    # --- å„²å­˜æœ€çµ‚æ¨¡å‹ ---
    print("\n--- æ­£åœ¨å„²å­˜æœ€çµ‚çš„æœ€ä½³æ¨¡å‹ ---")
    final_model_path = training_args.output_dir
    trainer.save_model(final_model_path)
    processor.save_pretrained(final_model_path)
    print(f"\næœ€çµ‚æ¨¡å‹å·²å„²å­˜è‡³ï¼š{final_model_path}")


if __name__ == "__main__":
    """
    ğŸš€ åŸ·è¡Œèªªæ˜ï¼š
    ------------------------------------------------------------------------------
    1. ç¢ºä¿å·²å®‰è£ CUDA ç‰ˆæœ¬çš„ PyTorch
    2. ç¢ºä¿å·²ä½¿ç”¨ `huggingface-cli login` ç™»å…¥
    3. ç¢ºä¿æœ‰è¶³å¤ çš„ GPU è¨˜æ†¶é«” (å»ºè­° 8GB+)
    4. åŸ·è¡Œå‰å»ºè­°é‡æ–°å•Ÿå‹•é›»è…¦ï¼Œç¢ºä¿ç³»çµ±è™•æ–¼ä¹¾æ·¨ç‹€æ…‹
    
    ğŸ“Š é æœŸè¼¸å‡ºï¼š
    - è¨“ç·´é€²åº¦æ¢ï¼š0/10 â†’ 10/10
    - è©•ä¼°æŒ‡æ¨™ï¼šCER, semantic_similarity, combined_score
    - æ¨¡å‹ä¿å­˜ï¼šwhisper-small-zh-finetune-zh-final/
    """
    main()
